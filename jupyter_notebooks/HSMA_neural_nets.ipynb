{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - TensorFlow neural net\n",
    "\n",
    "In this workbook we build a neural network to predict survival. The two common frameworks used for neural networks (as of 2020) are TensorFlow and PyTorch. Both are excellent frameworks. TensorFlow frequently requires fewer lines of code, but PyTorch is more natively Python in its syntax. Here we use TensorFlow and keras' which is integrated into TensorFlow and makes it simpler and faster to build TensorFlow models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow\n",
    "\n",
    "To install TensorFlow as a new environment in Anaconda type the following from a terminal (if you have already set up o the 'Titanic' environment you do not need to do this - use the Titanic environment. Also if you are using Colab then you need not do anything as Colab also has everything you need already built in).\n",
    "\n",
    "`conda create -n tensorflow tensorflow && conda install -n tensorflow scikit-learn pandas matplotlib`\n",
    "\n",
    "Or PIP install with `pip install tensorflow`\n",
    "\n",
    "Then from Anaconda Navigator, select the Titanic or TensorFlow environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network unit - a neuron or perceptron\n",
    "\n",
    "The building block of a neural network is a neuron, which is essentially the same as the 'perceptron' described by Frank Rosenblatt in 1958.\n",
    "\n",
    "The neuron, or perceptron, takes inputs *X* and weights *W* (each individual input has a weight; a bias weight is also introduced by creating a dummy input with value 1). The neuron sums the *input x weight* and passes the output to an activation function. The simplest activation function is a step function, whereby if the output is >0 the output of the activation function is 1, otherwise the output is 0.\n",
    "\n",
    "![](./images/perceptron.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Having understood a neuron - which calculates the weighted sum of its inputs and passes it through an activation function, neural networks are easy(ish)!\n",
    "\n",
    "They are 'just' a network of such neurons, where the output of one becomes one of the inputs to the neurons in the next layer.\n",
    "\n",
    "This allows any complexity of function to be mimicked by a neural network (so long as the network includes a non-linear activation function, like ReLU - see below).\n",
    "\n",
    "Note the output layer may be composed of a single neuron, to predict a single value or single probability, or may be multiple neurons, to predict multiple values or multiple probabilities.\n",
    "\n",
    "![](./images/net_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Each neuron calculates the weighted sum of its inputs and passes that sum to an activation function. The two simplest functions are:\n",
    "\n",
    "* *Linear*: The weighted output is passed forward with no change.\n",
    "\n",
    "* *Step*: The output of the activation function is 0 or 1 depending on whether a threshold is reached.\n",
    "\n",
    "Other common activation functions are:\n",
    "\n",
    "* *Sigmoid*: Scales output 0-1 using a logistic function. Note that our simple perceptron becomes a logistic regression model if we use a sigmoid activation function. The sigmoid function is often used to produce a probability output at the final layer.\n",
    "\n",
    "* *tanh*: Scales output -1 to 1. Commonly used in older neural network models. Not commonly used now.\n",
    "\n",
    "* *ReLU (rectifying linear unit)*: Simply converts all negative values to zero, and leaves positive values unchanged. This very simple method is very common in deep neural networks, and is sufficient to allow networks to model non-linear functions.\n",
    "\n",
    "* *Leaky ReLU* and *Exponential Linear Unit (ELU)*: Common modifications to ReLU that do not have such a hard constrain on negative inputs. Try them out as replacements to ReLU.\n",
    "\n",
    "* *Maxout*: A generalised activation function that can model a complex non-linear activation function. \n",
    "\n",
    "* *SoftMax*: SoftMax is the final layer to use if you wish to normalise probability outputs from a network which has multiple class outputs.\n",
    "\n",
    "![](./images/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Loss functions are critical to neural networks as they provide the measure by which the neural network is in error, allowing modification of the network to reduce error.\n",
    "\n",
    "The most common loss functions are:\n",
    "\n",
    "* *Mean Squared Error Loss*: Common loss function for regression (predicting values rather than class).\n",
    "\n",
    "* *Cross Entropy Loss*: Common loss function for classification. *Binary Cross Entropy Loss* is used when the output is a binary classifier (like survive/die in the Titanic model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do neural networks learn? Backpropagation and optimisation\n",
    "\n",
    "*Backpropagation* is the process by which the final loss is distributed back through the network, allowing each weight to be updated in proportion to its contribution to the final error.\n",
    "\n",
    "For more on backpropagation see: https://youtu.be/Ilg3gGewQ5U\n",
    "\n",
    "For deeper maths on backpropagation see: https://youtu.be/tIeHLnjs5U8\n",
    "\n",
    "*Optimisation* is the step-wise process by which weights are updated. The basic underlying method, *gradient descent*, is that weights are adjusted in the direction that improves fit, and that weights are adjust more when the gradient (how much the output changes with each unit change to the weight) is higher.\n",
    "\n",
    "Common optimisers used are:\n",
    "\n",
    "* *Stochastic gradient descent*: Updates gradients based on single samples. Can be inefficient, so can be modified to use gradients based on a small batch (e.g. 8-64) of samples. *Momentum* may also be added to avoid becoming trapped in local minima.\n",
    "\n",
    "* *RMSprop*: A 'classic' benchmark optimiser. Adjusts steps based on a weighted average of all weight gradients.\n",
    "\n",
    "* *Adam*: The most common optimiser used today. Has complex adaptive momentum for speeding up learning.\n",
    "\n",
    "For more on optimisers see: https://youtu.be/mdKjMPmcWjY\n",
    "\n",
    "For a short video on momentum see: https://youtu.be/6iwvtzXZ4Mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network - the practicalities\n",
    "\n",
    "The training process of a neural network consists of three general phases which are repeated across all the data. All of the data is passed through the network multiple times (the number of iterations, which may be as few as 3-5 or may be 1000+). The three phases are:\n",
    "\n",
    "1. Pass training X data to the network and predict y\n",
    "\n",
    "1. Calculate the 'loss' (error) between the predicted and observed (actual) values of y\n",
    "\n",
    "1. Backpropagate the loss and update the weights (the job of the optimiser).\n",
    "\n",
    "The learning is repeated until maximum accuracy is achieved (but keep an eye on accuracy of test data as well as training data as the network may develop significant over-fitting to training data unless steps are taken to offset the potential for over-fitting, such as use of 'drop-out' layers described below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "\n",
    "The most common fully connected architecture design is to have the same number of neurons in each layer, and adjust that number and the number of layers. This makes exploring the size of the neural net relatively easy (if sometimes slow). \n",
    "\n",
    "As a rough guide - the size of the neural net should be increased until it over-fits data (increasing accuracy of training data with reducing accuracy of test data), and then use a form of *regularisation* to reduce the over-fitting (we will go through this process below).\n",
    "\n",
    "Some common architecture designs, which may be mixed in a single larger network, are:\n",
    "\n",
    "* *Fully connected*: The output of each neuron goes to all neurons in the next layer.\n",
    "\n",
    "* *Convolutional*: Common in image analysis. Small 'mini-nets' that look for patterns across the data - like a 'sliding window', but that can look at the whole picture at the same time. May also be used, for example, in time series to look for fingerprints of events anywhere in the time series.\n",
    "\n",
    "* *Recurrent*: Introduce the concept of some (limited) form of memory into the network - at any one time a number of input steps are effecting the network output. Useful, for example, in sound or video analysis.\n",
    "\n",
    "* *Transformers*: Sequence-to-sequence architecture. Convert sequences to sequences (e.g. translation).\n",
    "\n",
    "* *Embedding*: Converts a categorical value to a vector of numbers, e.g. word-2-vec converts words to vectors such that similar meaning words are positioned close together.\n",
    "\n",
    "* *Encoding*: Reduce many input features to fewer. This 'compresses' the data. De-coding layers may convert back to the original data.\n",
    "\n",
    "* *Generative*: Rather than regression, or classification, generative networks output some form of synthetic data (such as fake images; see https://www.thispersondoesnotexist.com/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "Also see the excellent introductory video (20 minutes) from 3brown1blue: https://youtu.be/aircAruvnKk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TensorFlow sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scale data\n",
    "\n",
    "In neural networks it is common to to scale input data 0-1 rather than use standardisation (subtracting mean and dividing by standard deviation) of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net\n",
    "\n",
    "Here we use the `sequential` method to set up a TensorFlow neural network. This simpler method assumes each layer occurs in sequence. Though simpler, it lacks some flexibility.\n",
    "\n",
    "We will put construction of the neural net into a separate function.\n",
    "\n",
    "The neural net is a relatively simple network. The inputs are connected to two hidden layers (of 240 and 50 nodes) before being connected to two output nodes corresponding to each class (died and survived). It also contains some useful additions (batch normalisation and dropout) as described below.\n",
    "\n",
    "The layers of the network are:\n",
    "\n",
    "1) An input layer (which does not need to be defined) \n",
    "\n",
    "2) A fully-connected (dense) layer.This is defined by the number of inputs (the number of input features) and the number of outputs. We will expand out feature data set up to 240 outputs. The output of the layer uses ReLU  (rectified linear unit) activation. ReLU activation is most common for the inner layers of a neural network. Negative input values are set to zero. Positive input values are left unchanged.\n",
    "\n",
    "5) A second fully connected layer which reduces the network down to 50 nodes. This again uses ReLU activation and is followed by batch normalisation, and dropout layers.\n",
    "\n",
    "7) A final fully connected linear layer of one nodes (more nodes could be used for more classes, in which case use `softmax` activation and `categorical_crossentropy` in the loss function).\n",
    "\n",
    "The output of the net is the probability of surviving (usually a probability of >= 0.5 will be classes as 'survived')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.0, \n",
    "             learning_rate=0.003):\n",
    "    \n",
    "    \"\"\"Make TensorFlow neural net\"\"\"\n",
    "    \n",
    "    # Clear Tensorflow \n",
    "    K.clear_session()\n",
    "    \n",
    "    # Set up neural net\n",
    "    net = Sequential()\n",
    "    \n",
    "    # Add hidden hidden_layers using a loop\n",
    "    for i in range(hidden_layers):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            hidden_layer_neurones, \n",
    "            input_dim=number_features,\n",
    "            activation='relu'))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(dropout))\n",
    "    \n",
    "    # Add final sigmoid activation output\n",
    "    net.add(Dense(1, activation='sigmoid'))    \n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    \n",
    "    net.compile(loss='binary_crossentropy', \n",
    "                optimizer=opt, \n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary of the model structure\n",
    "\n",
    "Here we will create a model with 10 input features and show the structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_np, y_np, test_size = 0.25, random_state=42)\n",
    "\n",
    "# Scale X data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy of trainign and test data fits\"\"\"    \n",
    "    \n",
    "    ### Get accuracy of fit to training data\n",
    "    probability = model.predict(X_train_sc)\n",
    "    y_pred_train = probability >= 0.5\n",
    "    y_pred_train = y_pred_train.flatten()\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    \n",
    "    ### Get accuracy of fit to test data\n",
    "    probability = model.predict(X_test_sc)\n",
    "    y_pred_test = probability >= 0.5\n",
    "    y_pred_test = y_pred_test.flatten()\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "    # Show acuracy\n",
    "    print (f'Training accuracy {accuracy_train:0.3f}')\n",
    "    print (f'Test accuracy {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict):\n",
    "    acc_values = history_dict['accuracy']\n",
    "    val_acc_values = history_dict['val_accuracy']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training history\n",
    "\n",
    "`history` is a dictionary containing data collected suring training. Let's take a look at the keys in this dictionary (these are the metrics monitored during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving fit by avoiding or reducing-over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Reduce complexity of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features,\n",
    "                hidden_layers=1,\n",
    "                hidden_layer_neurones=32)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Add dropout\n",
    "\n",
    "Using dropout, in each trainign epoch a random selection of weights are not used (the sleection changes from epoch to epoch). When predicting, after fitting, all weights are used.\n",
    "\n",
    "Dropout values of 0.2 to 0.5 are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features,\n",
    "                dropout=0.5)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) L1 and L2 regularisation\n",
    "\n",
    "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "\n",
    "Both reduce the weights associated with features in the network, and increase the weight of the bias so that all values are pulled closer to the average. L1 regularisation tends to pull fewer weights down, but reduces them to a greater degree, whereas L2 regularisation reduces all weights. \n",
    "\n",
    "L1 and L2 regularisation are commonly used in regression. They are now less commonly used in neural networks. Here we will just mention them rather than use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Combination of the above and with automatic early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.h5', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=25, restore_best_weights=True)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(\n",
    "    number_features,\n",
    "    hidden_layers=2,\n",
    "    hidden_layer_neurones=64,\n",
    "    dropout=0.3)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reloading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('titanic_tf_model.h5')\n",
    "\n",
    "# Load and use saved model - we need to first set up a model\n",
    "restored_model = keras.models.load_model('titanic_tf_model.h5')\n",
    " \n",
    "# Predict classes as normal\n",
    "predicted_proba = restored_model.predict(X_test_sc)\n",
    "\n",
    "# Show examples of predicted probability\n",
    "print(predicted_proba[0:5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(restored_model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

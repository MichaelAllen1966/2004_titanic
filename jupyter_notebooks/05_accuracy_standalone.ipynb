{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy measurements in machine learning\n",
    "\n",
    "The most common measurement in machine learning is ‘accuracy, that is the proportion of cases where the classification is correct. This may at times not be the best measurement. For example, imagine you wish to detect who is carrying the back death among people coming through an airport, by measuring a range of features (such as body temperature, or the presence of a particular skin rash pattern). Now imagine that one in a thousand people are carriers of black death, who if allowed through will start off a major epidemic. We may then optimise a machine learning algorithm and it reports 99.9% accuracy. Impressive! Or is it? The algorithm may have simply classed everybody as not having black death. It will be right 999 times out of 1,000. But at the same time it will miss every single case of black death that you intended to detect. This is an extreme example of where we need more sophisticated measurements of accuracy. In this particular case, we probably want to measure sensitivity and specificity. Sensitivity measures the proportion of positive cases (black death) correctly classified, while specificity measures the proportion of negative cases (no black death) correctly classified. We will later see how we might change machine learning thresholds to adjust the balance between sensitivity and specificity.\n",
    "\n",
    "Sensitivity and specificity are not the only measures (though they are common in health diagnosis problems). Below are a range of measures that our function will calculated (given the observed and predicted class values of a range of cases). In addition to sensitivity and specificity, common measures used in machine learning are precision, recall and f1 (a combination of precision and recall).\n",
    "\n",
    "The full list of measures that our function will return (in the form of a dictionary) is:\n",
    "\n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "\n",
    "Please note that these measures are for a ‘binomial’ classification problem. That is where there are two alternatives for each case (e.g. survived or not-survived on the Titanic). Where there are more than two possible classes, a confusion matrix is most commonly used (e.g. see https://pythonhealthcare.org/2018/04/21/77-machine-learning-visualising-accuracy-and-error-in-a-classification-model-with-a-confusion-matrix/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    false_negatives = (predicted_negatives == 1) & (observed_negatives == 0)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_negatives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed_positive_rate 0.4\n",
      "observed_negative_rate 0.6\n",
      "predicted_positive_rate 0.4\n",
      "predicted_negative_rate 0.6\n",
      "accuracy 0.8\n",
      "precision 0.75\n",
      "recall 0.75\n",
      "f1 0.75\n",
      "sensitivity 0.75\n",
      "specificity 0.833\n",
      "positive_likelihood 4.5\n",
      "negative_likelihood 0.3\n",
      "false_positive_rate 0.167\n",
      "false_negative_rate 0.25\n",
      "true_positive_rate 0.75\n",
      "true_negative_rate 0.833\n",
      "positive_predictive_value 0.75\n",
      "negative_predictive_value 0.833\n"
     ]
    }
   ],
   "source": [
    "# Create two lists to test function\n",
    "observed =  [0, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "predicted = [0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "\n",
    "# Call calculate_accuracy function\n",
    "accuracy = calculate_accuracy(observed, predicted)\n",
    "\n",
    "# Print results up to three decimal places\n",
    "for key, value in accuracy.items():\n",
    "    print (key, \"{0:0.3}\".format(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
